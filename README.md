# NoMoreNoise
Hi professore, I and Mattia (my co-worker) have worked a lot for this homework, we have tried to reach the baseline in various way. Former we tried to an hyper-parameter search on the model, changing the layer, dimensions, dropout and the gnn model. We tried also to use a lot of differente loss, implemented by us, as SOP, adaptiveSOP, NoisyCE, Asymmetric CE, Symmetric CE. We tried first gcn, and then we finally reached the baseline with the gin-virtual, with 3 layers, 300 as dimension, and 0.5 as dropout for A,B, D and for C 0.7, and we reached 0.833 . 
Latter we tried multiple days to make the model work and reach the real baseline, but we struggled a lot and decide to change out approach. We did a fine-tune on an existing [model](https://github.com/cminuttim/Learning-with-Noisy-Graph-Labels-Competition-IJCNN_2025), and did the fine-tuning on each dataset, with the Noisy CE with noisy probability = 0.2 . 
